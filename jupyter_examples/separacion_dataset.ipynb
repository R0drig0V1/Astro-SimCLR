{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1430/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "import sys\n",
    "import pickle\n",
    "import random \n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "\n",
    "from utils.dataset import img_float2int, feature_names, label_names\n",
    "\n",
    "from utils.transformations import SimCLR_augmentation\n",
    "from utils.transformations import Astro_augmentation\n",
    "from utils.transformations import Astro_augmentation_v2\n",
    "from utils.transformations import Astro_augmentation_v3\n",
    "from utils.transformations import Jitter_astro\n",
    "from utils.transformations import Jitter_simclr\n",
    "from utils.transformations import Crop_astro\n",
    "from utils.transformations import Crop_simclr\n",
    "from utils.transformations import Rotation\n",
    "from utils.transformations import Rotation_v2\n",
    "from utils.transformations import Gaussian_blur\n",
    "from utils.transformations import Resize_img\n",
    "from utils.transformations import RandomPerspective\n",
    "from utils.transformations import GridDistortion\n",
    "from utils.transformations import ElasticTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función que explora el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the type of the objects inside the input.\n",
    "def dataset_structure(data, space=''):\n",
    "\n",
    "    # Iteration along dictionary\n",
    "    for keys in data:\n",
    "\n",
    "        # If the object is other dictionary\n",
    "        if(type(data[keys]) is dict):\n",
    "\n",
    "            print('{0}-{1}:'.format(space,keys))\n",
    "            dataset_structure(data[keys], space + '\\t')\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Type of the object\n",
    "            type_obj = type(data[keys]).__name__\n",
    "\n",
    "            # Extracts an example of the elements into the object\n",
    "            example = data[keys][0]\n",
    "            type_example = type(example).__name__\n",
    "\n",
    "            # If the element is an ndarray, its shape is printed\n",
    "            if(type_example=='ndarray'):\n",
    "                shape_example = str(np.shape(example))\n",
    "            else:\n",
    "                shape_example = 'Non-ndarray'\n",
    "\n",
    "            # Print the information (length and type) of the structure inside\n",
    "            # the dictionary\n",
    "            print(\"{0}-{1:<10}N={2:<10}Type:{3}\".format(space, keys, len(data[keys]), type_obj), end=\"\")\n",
    "            print(\"\\t-->\\tType:{1:<10}Shape:{2:<10}\".format(space, type_example, shape_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Train:\n",
      "\t-images    N=72710     Type:ndarray\t-->\tType:ndarray   Shape:(63, 63, 3)\n",
      "\t-labels    N=72710     Type:ndarray\t-->\tType:int64     Shape:Non-ndarray\n",
      "\t-features  N=72710     Type:ndarray\t-->\tType:ndarray   Shape:(26,)     \n",
      "-Validation:\n",
      "\t-images    N=500       Type:ndarray\t-->\tType:ndarray   Shape:(63, 63, 3)\n",
      "\t-labels    N=500       Type:ndarray\t-->\tType:int64     Shape:Non-ndarray\n",
      "\t-features  N=500       Type:ndarray\t-->\tType:ndarray   Shape:(26,)     \n",
      "-Test:\n",
      "\t-images    N=500       Type:ndarray\t-->\tType:ndarray   Shape:(63, 63, 3)\n",
      "\t-labels    N=500       Type:ndarray\t-->\tType:int64     Shape:Non-ndarray\n",
      "\t-features  N=500       Type:ndarray\t-->\tType:ndarray   Shape:(26,)     \n"
     ]
    }
   ],
   "source": [
    "# Dataset is loaded\n",
    "with open('../dataset/td_ztf_stamp_17_06_20.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Inner structure of dataset\n",
    "dataset_structure(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72710/72710 [5:31:38<00:00,  3.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 431.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 445.63it/s]\n"
     ]
    }
   ],
   "source": [
    "new_data = {}\n",
    "\n",
    "for subset in ['Train', 'Validation', 'Test']:\n",
    "    \n",
    "    new_data[subset] = {'images':[], 'features':[], 'labels':[]}\n",
    "    L = len(data[subset]['labels'])\n",
    "    idx = 0\n",
    "    \n",
    "    rep_list = []    \n",
    "    \n",
    "    for i in tqdm(range(L)):\n",
    "        if (i not in rep_list):\n",
    "            for j in range(i+1, L):\n",
    "                #if (j not in rep_list):\n",
    "                if (np.linalg.norm(data[subset]['images'][j] - data[subset]['images'][i]) < 0.00001):\n",
    "                    rep_list.append(j)\n",
    "    \n",
    "        \n",
    "\n",
    "        if (i not in rep_list):\n",
    "            new_data[subset]['images'].append(data[subset]['images'][i])\n",
    "            new_data[subset]['features'].append(data[subset]['features'][i])\n",
    "            new_data[subset]['labels'].append(data[subset]['labels'][i])\n",
    "        \n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guarda el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save dataset        \u001b[39;00m\n\u001b[1;32m      2\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd_ztf_stamp_17_06_20_red.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mnew_data\u001b[49m, file)\n\u001b[1;32m      4\u001b[0m file\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Save dataset        \n",
    "file = open('td_ztf_stamp_17_06_20_red.pkl', 'wb')\n",
    "pickle.dump(new_data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_dataset(data):\n",
    "    \n",
    "    counter_train = collections.Counter(data['Train']['labels'])\n",
    "    counter_validation = collections.Counter(data['Validation']['labels'])\n",
    "    counter_test = collections.Counter(data['Test']['labels'])\n",
    "\n",
    "    print('Train:')\n",
    "    for key, value in counter_train.items():\n",
    "        print(f'\\t{key}:{value}')\n",
    "    \n",
    "    print('Validation:')\n",
    "    for key, value in counter_validation.items():\n",
    "        print(f'\\t{key}:{value}')\n",
    "\n",
    "    print('Test:')\n",
    "    for key, value in counter_test.items():\n",
    "        print(f'\\t{key}:{value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separa dataset para SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/td_ztf_stamp_17_06_20_red.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\t2:14517\n",
      "\t0:14542\n",
      "\t1:1396\n",
      "\t4:10528\n",
      "\t3:9599\n",
      "Validation:\n",
      "\t0:100\n",
      "\t1:100\n",
      "\t2:100\n",
      "\t3:100\n",
      "\t4:100\n",
      "Test:\n",
      "\t0:100\n",
      "\t1:100\n",
      "\t2:100\n",
      "\t3:100\n",
      "\t4:100\n"
     ]
    }
   ],
   "source": [
    "class_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partion_dic(data, p):\n",
    "    \n",
    "    partion = {}\n",
    "    N = len(data['labels'])\n",
    "\n",
    "    idx = random.sample(range(N), int(N*p))\n",
    "    \n",
    "    for key, val in data.items():\n",
    "        partion[key] = [data[key][i] for i in idx]\n",
    "    \n",
    "    return partion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dic(data, p):\n",
    "    \n",
    "    partion_1 = {}\n",
    "    partion_2 = {}\n",
    "    \n",
    "    N = len(data['labels'])\n",
    "    idx = random.sample(range(N), int(N*p))\n",
    "    not_idx = np.delete(np.arange(N), idx)\n",
    "    \n",
    "    for key, val in data.items():\n",
    "        partion_1[key] = [data[key][i] for i in idx]\n",
    "        partion_2[key] = [data[key][i] for i in not_idx]\n",
    "        \n",
    "    \n",
    "    return partion_1, partion_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(data):\n",
    "    \n",
    "    N = len(data['labels'])\n",
    "    balanced_data = data\n",
    "    counter = collections.Counter(data['labels'])\n",
    "    \n",
    "    deficit = {key: max(counter.values()) - val for key, val in counter.items()}\n",
    "\n",
    "    for key, value in counter.items():\n",
    "        print(f'\\t{key}:{value}')\n",
    "       \n",
    "    \n",
    "    for key, val in deficit.items():\n",
    "        \n",
    "        idx = np.random.choice([i for i in range(N) if key==data['labels'][i]], val)\n",
    "        \n",
    "        balanced_data['images'] += [data['images'][i] for i in idx]\n",
    "        balanced_data['features'] += [data['features'][i] for i in idx]\n",
    "        balanced_data['labels'] += [data['labels'][i] for i in idx]\n",
    "        \n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t2:14517\n",
      "\t0:14542\n",
      "\t1:1396\n",
      "\t4:10528\n",
      "\t3:9599\n",
      "Train:\n",
      "\t2:14542\n",
      "\t0:14542\n",
      "\t1:14542\n",
      "\t4:14542\n",
      "\t3:14542\n",
      "Validation:\n",
      "\t0:100\n",
      "\t1:100\n",
      "\t2:100\n",
      "\t3:100\n",
      "\t4:100\n",
      "Test:\n",
      "\t0:100\n",
      "\t1:100\n",
      "\t2:100\n",
      "\t3:100\n",
      "\t4:100\n"
     ]
    }
   ],
   "source": [
    "data_sup_100 = {'Train': balance(copy.deepcopy(data['Train'])), 'Validation': data['Validation'], 'Test': data['Test']}\n",
    "class_dataset(data_sup_100)\n",
    "\n",
    "# Save dataset        \n",
    "file = open('td_ztf_stamp_17_06_20_sup_100.pkl', 'wb')\n",
    "pickle.dump(data_sup_100, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0:1496\n",
      "\t4:1086\n",
      "\t3:905\n",
      "\t2:1428\n",
      "\t1:143\n",
      "Train:\n",
      "\t0:1496\n",
      "\t4:1496\n",
      "\t3:1496\n",
      "\t2:1496\n",
      "\t1:1496\n",
      "Validation:\n",
      "\t0:100\n",
      "\t1:100\n",
      "\t2:100\n",
      "\t3:100\n",
      "\t4:100\n",
      "Test:\n",
      "\t0:100\n",
      "\t1:100\n",
      "\t2:100\n",
      "\t3:100\n",
      "\t4:100\n"
     ]
    }
   ],
   "source": [
    "p = 0.1\n",
    "data_sup_10 = {'Train': balance(copy.deepcopy(partion_dic(data['Train'], p))), 'Validation': data['Validation'], 'Test': data['Test']}\n",
    "\n",
    "class_dataset(data_sup_10)\n",
    "\n",
    "# Save dataset        \n",
    "file = open('td_ztf_stamp_17_06_20_sup_10.pkl', 'wb')\n",
    "pickle.dump(data_sup_10, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t4:110\n",
      "\t2:162\n",
      "\t3:73\n",
      "\t0:146\n",
      "\t1:14\n",
      "Train:\n",
      "\t4:162\n",
      "\t2:162\n",
      "\t3:162\n",
      "\t0:162\n",
      "\t1:162\n",
      "Validation:\n",
      "\t0:100\n",
      "\t1:100\n",
      "\t2:100\n",
      "\t3:100\n",
      "\t4:100\n",
      "Test:\n",
      "\t0:100\n",
      "\t1:100\n",
      "\t2:100\n",
      "\t3:100\n",
      "\t4:100\n"
     ]
    }
   ],
   "source": [
    "p = 0.01\n",
    "data_sup_1 = {'Train': balance(copy.deepcopy(partion_dic(data['Train'], p))), 'Validation': data['Validation'], 'Test': data['Test']}\n",
    "class_dataset(data_sup_1)\n",
    "\n",
    "# Save dataset        \n",
    "file = open('td_ztf_stamp_17_06_20_sup_1.pkl', 'wb')\n",
    "pickle.dump(data_sup_1, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
